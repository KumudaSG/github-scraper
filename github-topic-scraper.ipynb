{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# github-topic-scraper\n",
    "\n",
    "In this project, I try to get the top 20 repositories' data for each of the first 30 topics listed on github.com\n",
    "\n",
    "URL used: https://github.com/topics <br>\n",
    "Tools used: Python (requests, BeautifulSoup, pandas) <br>\n",
    "\n",
    "Final desired output: <br>\n",
    "\n",
    "topics.csv file in format:  topic_title | topic_description | topic_page_url\n",
    "\n",
    "For each topic, create a file with top 20 repos. <br>\n",
    "Format: username | repository_name | no. of stars | repository_url <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Outline\n",
    "\n",
    " 1. Use the requests library to load the topics page <br>\n",
    " 2. Create a BeautifulSoup object to parse data <br>\n",
    " 3. Find tags for each of the data elements we want to scrape. <br>\n",
    " 4. Use the above tags to get information. <br>\n",
    " 5. Create a pandas dataframe using the above information <br>\n",
    " 6. Convert the dataframe to a CSV file and save. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the topics page and information for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics():\n",
    "    response_top = requests.get(\"https://github.com/topics\")\n",
    "    if response_top.status_code != 200:\n",
    "        raise Exception ('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response_top.text,'html.parser')\n",
    "    \n",
    "    #get all titles\n",
    "    topic_title = []\n",
    "    topic_title_tags = doc.find_all('p' ,{'class' : 'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "    for i in topic_title_tags:\n",
    "        topic_title.append(i.text)  # i.text : line gives the text between the tags.\n",
    "        \n",
    "    #get all desc\n",
    "    topic_desc = []\n",
    "    topic_desc_tags = doc.find_all('p',{'class':'f5 color-fg-muted mb-0 mt-1'})\n",
    "    for j in topic_desc_tags:\n",
    "        topic_desc.append(j.text.strip()) #strip function removes space before and after the line.\n",
    "        \n",
    "    #get all urls\n",
    "    topic_urls = []\n",
    "    base = 'https://github.com'\n",
    "    topic_urls_tags = doc.find_all('a',{'class':'no-underline flex-1 d-flex flex-column'})\n",
    "    for k in topic_urls_tags:\n",
    "        topic_urls.append(base + k['href']) #only gets the href part.\n",
    "    d = {\n",
    "    'title' : topic_title,\n",
    "    'descriptions': topic_desc,\n",
    "    'url' : topic_urls }\n",
    "    \n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get information from each repository in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag,star_count):\n",
    "    base_url = \"https://github.com\"\n",
    "    a_tag = h3_tag.find_all('a')\n",
    "    username = a_tag[0].text.strip()\n",
    "    reponame = a_tag[1].text.strip()\n",
    "    repo_url = base_url +a_tag[1]['href']\n",
    "    star_num = parse_star_count(star_count)\n",
    "    return username,reponame,repo_url,star_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_url):\n",
    "    resp = requests.get(topic_url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception ('Failed to load page {}'.format(topic_url))\n",
    "    topic_doc = BeautifulSoup(resp.text,'html.parser')\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "    # we're trying to find the a tags under these but they dont have the class attribute.\n",
    "    # so we will relate them to these tags using other methods.\n",
    "    star_tags = topic_doc.find_all('span', {'class' : 'Counter js-social-count'})\n",
    "    \n",
    "    \n",
    "    topic_repos_dict = {\n",
    "    'username' : [],\n",
    "    'repo_name': [],\n",
    "    'stars': [],\n",
    "    'repo_url' : []  \n",
    "    }\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],star_tags[i].text)\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[3])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[2])\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a folder 'data' with CSV files for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_repos():\n",
    "    topics_df = get_topics()\n",
    "    for index, row in topics_df.iterrows():\n",
    "        #Creating a folder\n",
    "        os.makedirs('data',exist_ok = True)\n",
    "        fname = row['title'] + '.csv'\n",
    "        if not(os.path.exists('data/'+fname)):\n",
    "            print(\"Scraping top 20 repos for topic \"+row['title'])\n",
    "            repos_df = get_topic_repos(row['url'])\n",
    "            repos_df.to_csv('data/'+fname, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top 20 repos for topic 3D\n",
      "Scraping top 20 repos for topic Ajax\n",
      "Scraping top 20 repos for topic Algorithm\n",
      "Scraping top 20 repos for topic Amp\n",
      "Scraping top 20 repos for topic Android\n",
      "Scraping top 20 repos for topic Angular\n",
      "Scraping top 20 repos for topic Ansible\n",
      "Scraping top 20 repos for topic API\n",
      "Scraping top 20 repos for topic Arduino\n",
      "Scraping top 20 repos for topic ASP.NET\n",
      "Scraping top 20 repos for topic Atom\n",
      "Scraping top 20 repos for topic Awesome Lists\n",
      "Scraping top 20 repos for topic Amazon Web Services\n",
      "Scraping top 20 repos for topic Azure\n",
      "Scraping top 20 repos for topic Babel\n",
      "Scraping top 20 repos for topic Bash\n",
      "Scraping top 20 repos for topic Bitcoin\n",
      "Scraping top 20 repos for topic Bootstrap\n",
      "Scraping top 20 repos for topic Bot\n",
      "Scraping top 20 repos for topic C\n",
      "Scraping top 20 repos for topic Chrome\n",
      "Scraping top 20 repos for topic Chrome extension\n",
      "Scraping top 20 repos for topic Command line interface\n",
      "Scraping top 20 repos for topic Clojure\n",
      "Scraping top 20 repos for topic Code quality\n",
      "Scraping top 20 repos for topic Code review\n",
      "Scraping top 20 repos for topic Compiler\n",
      "Scraping top 20 repos for topic Continuous integration\n",
      "Scraping top 20 repos for topic COVID-19\n",
      "Scraping top 20 repos for topic C++\n"
     ]
    }
   ],
   "source": [
    "scrape_top_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Future Work\n",
    "\n",
    "References:\n",
    "- https://jovian.com/\n",
    "- https://pypi.org/project/beautifulsoup4/\n",
    "- https://realpython.com/python-web-scraping-practical-introduction/\n",
    "- https://www.w3schools.com/python/pandas/default.asp\n",
    "\n",
    "<br>\n",
    "Ideas for Future Work: <br>\n",
    "- Set up a scheduled script using tools like Task Scheduler to automatically run the scraping and data collection process at specific intervals. This ensures that the dataset stays up to date without manual intervention. <br>\n",
    "- Implement more robust data cleaning techniques using pandas to handle missing or inconsistent data. Explore different strategies to impute or remove outliers and enhance the overall data quality <br>\n",
    "- Utilize BeautifulSoup to extract more detailed information from the repository descriptions, such as sentiment analysis or keyword extraction. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
